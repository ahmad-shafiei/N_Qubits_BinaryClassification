{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a6874bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A] Multiclass SVM (ZZ kernel) acc = 0.3755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10\\anaconda3\\envs\\qcomputing\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B] Hierarchical: combine 4 binaries acc = 0.3872\n",
      "[B] Hierarchical + meta-corrector acc = 0.3862\n",
      "[C] Multiclass MLP (polar_cross) acc = 0.2848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10\\anaconda3\\envs\\qcomputing\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Practical pipelines to improve 16-class (4-qubit) readout classification\n",
    "from fixed IQ points (one complex number per qubit per shot).\n",
    "\n",
    "Implements 3 practical strategies:\n",
    "  A) Baseline multiclass SVM (RBF or custom ZZ kernel)\n",
    "  B) Hierarchical decoding:\n",
    "       - 4 binary classifiers for q1..q4 (multi-output)\n",
    "       - combine to 16-class via bit packing\n",
    "       - optional meta-corrector to fix correlated errors using predicted probabilities\n",
    "  C) MLP discriminator (multiclass)\n",
    "\n",
    "Feature engineering options:\n",
    "  - encoding=\"cartesian\": [Re1, Im1, ..., Re4, Im4]  -> 8 dims\n",
    "  - encoding=\"polar\":     [amp1, sin(phi1), cos(phi1), ..., amp4, sin(phi4), cos(phi4)] -> 12 dims\n",
    "  - encoding=\"polar_cross\": polar + cross terms Re(z_i z_j*) and Im(z_i z_j*) for i<j -> 12 + 12 = 24 dims\n",
    "\n",
    "Scaling options:\n",
    "  - scaler=\"robust\" | \"standard\" | \"none\"\n",
    "\n",
    "Data files:\n",
    "  - dataset_X.txt (N,4) complex\n",
    "  - dataset_y.txt (N,4) bits [q1 q2 q3 q4]\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ---------------------------\n",
    "# Loading\n",
    "# ---------------------------\n",
    "def load_dataset(X_path=\"dataset_X.txt\", y_path=\"dataset_y.txt\"):\n",
    "    X = np.loadtxt(X_path, dtype=complex)   # (N,4)\n",
    "    y = np.loadtxt(y_path, dtype=int)       # (N,4)\n",
    "    return X, y\n",
    "\n",
    "def bits_to_int_labels(y_bits):\n",
    "    \"\"\"\n",
    "    Convert [q1,q2,q3,q4] -> integer 0..15, q1 LSB, q4 MSB.\n",
    "    \"\"\"\n",
    "    q1, q2, q3, q4 = (y_bits[:, k] for k in range(4))\n",
    "    return (q1 + 2*q2 + 4*q3 + 8*q4).astype(int)\n",
    "\n",
    "# ---------------------------\n",
    "# Feature engineering\n",
    "# ---------------------------\n",
    "def features_cartesian(Xc):\n",
    "    # (N,4) complex -> (N,8)\n",
    "    return np.hstack([Xc.real, Xc.imag])\n",
    "\n",
    "def features_polar(Xc):\n",
    "    # (N,4) complex -> (N,12): [amp, sin(phi), cos(phi)] per qubit\n",
    "    amp = np.abs(Xc)\n",
    "    phi = np.angle(Xc)\n",
    "    return np.hstack([amp, np.sin(phi), np.cos(phi)])\n",
    "\n",
    "def features_cross_terms(Xc):\n",
    "    \"\"\"\n",
    "    Cross terms from z_i * conj(z_j): capture relative phase & correlation\n",
    "    For each pair (i<j): produce Re and Im -> 2 * C(4,2) = 12 dims\n",
    "    \"\"\"\n",
    "    N = Xc.shape[0]\n",
    "    feats = []\n",
    "    for i, j in combinations(range(4), 2):\n",
    "        prod = Xc[:, i] * np.conjugate(Xc[:, j])\n",
    "        feats.append(prod.real.reshape(N, 1))\n",
    "        feats.append(prod.imag.reshape(N, 1))\n",
    "    return np.hstack(feats)  # (N,12)\n",
    "\n",
    "def make_features(Xc, encoding=\"cartesian\"):\n",
    "    if encoding == \"cartesian\":\n",
    "        return features_cartesian(Xc)\n",
    "    if encoding == \"polar\":\n",
    "        return features_polar(Xc)\n",
    "    if encoding == \"polar_cross\":\n",
    "        return np.hstack([features_polar(Xc), features_cross_terms(Xc)])\n",
    "    raise ValueError(\"encoding must be 'cartesian'|'polar'|'polar_cross'\")\n",
    "\n",
    "def make_scaler(scaler=\"robust\"):\n",
    "    if scaler == \"robust\":\n",
    "        return RobustScaler()\n",
    "    if scaler == \"standard\":\n",
    "        return StandardScaler()\n",
    "    if scaler == \"none\":\n",
    "        return None\n",
    "    raise ValueError(\"scaler must be 'robust'|'standard'|'none'\")\n",
    "\n",
    "# ---------------------------\n",
    "# ZZ kernel (optional)\n",
    "# ---------------------------\n",
    "from scipy.linalg import pinv\n",
    "\n",
    "def make_zz_kernel(gamma_single=0.05, gamma_pair=0.05):\n",
    "    \"\"\"\n",
    "    ZZ-inspired kernel for real feature vectors:\n",
    "        K = Π_k cos(γs (x_k - y_k))  ×  Π_{i<j} cos(γp (x_i x_j - y_i y_j))\n",
    "    \"\"\"\n",
    "    def zz_kernel(X, Y=None):\n",
    "        X = np.asarray(X, float)\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "        else:\n",
    "            Y = np.asarray(Y, float)\n",
    "\n",
    "        diff = X[:, None, :] - Y[None, :, :]\n",
    "        K_single = np.cos(gamma_single * diff).prod(axis=2)\n",
    "\n",
    "        K_pair = np.ones_like(K_single)\n",
    "        d = X.shape[1]\n",
    "        for i in range(d):\n",
    "            for j in range(i+1, d):\n",
    "                uX = X[:, i] * X[:, j]\n",
    "                uY = Y[:, i] * Y[:, j]\n",
    "                K_pair *= np.cos(gamma_pair * (uX[:, None] - uY[None, :]))\n",
    "\n",
    "        return K_single * K_pair\n",
    "\n",
    "    return zz_kernel\n",
    "\n",
    "# ============================================================\n",
    "# A) Baseline multiclass SVM\n",
    "# ============================================================\n",
    "def run_multiclass_svm(\n",
    "    encoding=\"cartesian\",\n",
    "    scaler=\"robust\",\n",
    "    kernel=\"rbf\",             # \"rbf\" or \"zz\"\n",
    "    gamma_single=0.05,\n",
    "    gamma_pair=0.05,\n",
    "    C=1.0,\n",
    "    test_size=0.25,\n",
    "    seed=42\n",
    "):\n",
    "    Xc, y_bits = load_dataset()\n",
    "    X = make_features(Xc, encoding=encoding)\n",
    "    y = bits_to_int_labels(y_bits)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    sc = make_scaler(scaler)\n",
    "    if kernel == \"rbf\":\n",
    "        clf = SVC(kernel=\"rbf\", C=C, gamma=\"scale\", decision_function_shape=\"ovr\")\n",
    "    elif kernel == \"zz\":\n",
    "        clf = SVC(kernel=make_zz_kernel(gamma_single, gamma_pair),\n",
    "                  C=C, decision_function_shape=\"ovr\")\n",
    "    else:\n",
    "        raise ValueError(\"kernel must be 'rbf' or 'zz'\")\n",
    "\n",
    "    if sc is None:\n",
    "        model = clf\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        model = Pipeline([(\"scaler\", sc), (\"clf\", clf)])\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return acc, cm\n",
    "\n",
    "# ============================================================\n",
    "# B) Hierarchical decoding (4 binary + meta-corrector)\n",
    "# ============================================================\n",
    "def run_hierarchical_decoding(\n",
    "    encoding=\"cartesian\",\n",
    "    scaler=\"robust\",\n",
    "    base_model=\"svm_rbf\",   # \"svm_rbf\" or \"mlp\"\n",
    "    meta_corrector=True,\n",
    "    test_size=0.25,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Step 1: train 4 binary classifiers: predict q1..q4 from full feature vector.\n",
    "    Step 2: combine predicted bits -> 16-class label.\n",
    "    Step 3 (optional): meta-corrector trained on predicted probabilities to correct errors.\n",
    "    \"\"\"\n",
    "\n",
    "    Xc, y_bits = load_dataset()\n",
    "    X = make_features(Xc, encoding=encoding)\n",
    "    y16 = bits_to_int_labels(y_bits)\n",
    "\n",
    "    X_train, X_test, y_train_bits, y_test_bits, y_train16, y_test16 = train_test_split(\n",
    "        X, y_bits, y16, test_size=test_size, random_state=seed, stratify=y16\n",
    "    )\n",
    "\n",
    "    sc = make_scaler(scaler)\n",
    "    if sc is not None:\n",
    "        sc.fit(X_train)\n",
    "        Xtr = sc.transform(X_train)\n",
    "        Xte = sc.transform(X_test)\n",
    "    else:\n",
    "        Xtr, Xte = X_train, X_test\n",
    "\n",
    "    # --- Train 4 binary models ---\n",
    "    bin_models = []\n",
    "    bin_proba_tr = []\n",
    "    bin_proba_te = []\n",
    "\n",
    "    for qi in range(4):\n",
    "        ytr = y_train_bits[:, qi]\n",
    "        yte = y_test_bits[:, qi]\n",
    "\n",
    "        if base_model == \"svm_rbf\":\n",
    "            m = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=True)\n",
    "        elif base_model == \"mlp\":\n",
    "            m = MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                              activation=\"relu\",\n",
    "                              max_iter=300,\n",
    "                              random_state=seed)\n",
    "        else:\n",
    "            raise ValueError(\"base_model must be 'svm_rbf' or 'mlp'\")\n",
    "\n",
    "        m.fit(Xtr, ytr)\n",
    "\n",
    "        # predicted probabilities for class=1\n",
    "        ptr = m.predict_proba(Xtr)[:, 1]\n",
    "        pte = m.predict_proba(Xte)[:, 1]\n",
    "\n",
    "        bin_models.append(m)\n",
    "        bin_proba_tr.append(ptr.reshape(-1, 1))\n",
    "        bin_proba_te.append(pte.reshape(-1, 1))\n",
    "\n",
    "    P_tr = np.hstack(bin_proba_tr)  # (Ntrain,4)\n",
    "    P_te = np.hstack(bin_proba_te)  # (Ntest,4)\n",
    "\n",
    "    # Hard bit decisions\n",
    "    pred_bits = (P_te >= 0.5).astype(int)\n",
    "    pred16 = bits_to_int_labels(pred_bits)\n",
    "\n",
    "    acc_base = accuracy_score(y_test16, pred16)\n",
    "\n",
    "    if not meta_corrector:\n",
    "        return acc_base, None\n",
    "\n",
    "    # --- Meta-corrector: learn mapping from probabilities to 16-class ---\n",
    "    # Use multinomial logistic regression on 4 probability features (or add more later)\n",
    "    meta = LogisticRegression(max_iter=2000, multi_class=\"multinomial\")\n",
    "    meta.fit(P_tr, y_train16)\n",
    "    pred16_meta = meta.predict(P_te)\n",
    "    acc_meta = accuracy_score(y_test16, pred16_meta)\n",
    "\n",
    "    return acc_base, acc_meta\n",
    "\n",
    "# ============================================================\n",
    "# C) Multiclass MLP discriminator\n",
    "# ============================================================\n",
    "def run_multiclass_mlp(\n",
    "    encoding=\"polar_cross\",\n",
    "    scaler=\"robust\",\n",
    "    hidden=(128, 64),\n",
    "    alpha=1e-4,\n",
    "    max_iter=400,\n",
    "    test_size=0.25,\n",
    "    seed=42\n",
    "):\n",
    "    Xc, y_bits = load_dataset()\n",
    "    X = make_features(Xc, encoding=encoding)\n",
    "    y = bits_to_int_labels(y_bits)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    sc = make_scaler(scaler)\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=hidden,\n",
    "                        activation=\"relu\",\n",
    "                        alpha=alpha,\n",
    "                        max_iter=max_iter,\n",
    "                        random_state=seed)\n",
    "\n",
    "    if sc is None:\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "    else:\n",
    "        model = Pipeline([(\"scaler\", sc), (\"clf\", clf)])\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return acc, cm\n",
    "\n",
    "# ============================================================\n",
    "# Convenience runner\n",
    "# ============================================================\n",
    "def main():\n",
    "    # 1) Baseline SVM (ZZ) - your current best idea\n",
    "    acc_zz, _ = run_multiclass_svm(\n",
    "        encoding=\"cartesian\",\n",
    "        scaler=\"robust\",\n",
    "        kernel=\"zz\",\n",
    "        gamma_single=0.05,\n",
    "        gamma_pair=0.05,\n",
    "        C=1.0\n",
    "    )\n",
    "    print(f\"[A] Multiclass SVM (ZZ kernel) acc = {acc_zz:.4f}\")\n",
    "\n",
    "    # 2) Hierarchical decoding\n",
    "    acc_bits, acc_meta = run_hierarchical_decoding(\n",
    "        encoding=\"cartesian\",\n",
    "        scaler=\"robust\",\n",
    "        base_model=\"svm_rbf\",\n",
    "        meta_corrector=True\n",
    "    )\n",
    "    print(f\"[B] Hierarchical: combine 4 binaries acc = {acc_bits:.4f}\")\n",
    "    print(f\"[B] Hierarchical + meta-corrector acc = {acc_meta:.4f}\")\n",
    "\n",
    "    # 3) Multiclass MLP with richer features\n",
    "    acc_mlp, _ = run_multiclass_mlp(\n",
    "        encoding=\"polar_cross\",\n",
    "        scaler=\"robust\",\n",
    "        hidden=(128, 64),\n",
    "        alpha=1e-4,\n",
    "        max_iter=500\n",
    "    )\n",
    "    print(f\"[C] Multiclass MLP (polar_cross) acc = {acc_mlp:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2509f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Contextual Bandit (Linear UCB) ===\n",
      "Feature mode : polar_cross\n",
      "Alpha        : 0.1\n",
      "Accuracy     : 0.3663\n",
      "\n",
      "=== Contextual Bandit (Linear UCB) ===\n",
      "Feature mode : polar_cross\n",
      "Alpha        : 0.3\n",
      "Accuracy     : 0.3595\n",
      "\n",
      "=== Contextual Bandit (Linear UCB) ===\n",
      "Feature mode : polar_cross\n",
      "Alpha        : 0.5\n",
      "Accuracy     : 0.3688\n",
      "\n",
      "=== Contextual Bandit (Linear UCB) ===\n",
      "Feature mode : polar_cross\n",
      "Alpha        : 1.0\n",
      "Accuracy     : 0.3510\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Contextual Bandit (Neural / Linear UCB style) for 4-qubit (16-class) readout.\n",
    "\n",
    "Offline setting:\n",
    "  - context x : feature vector extracted from IQ readout\n",
    "  - action a  : predicted 4-qubit state (0..15)\n",
    "  - reward r  : 1 if correct, 0 otherwise\n",
    "\n",
    "This implementation uses Linear UCB per class (action).\n",
    "Fully compatible with fixed dataset_X.txt / dataset_y.txt.\n",
    "\n",
    "Why this works:\n",
    "  - uncertainty-aware decision making\n",
    "  - robust to class overlap\n",
    "  - RL-inspired without requiring environment dynamics\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Data loading and labels\n",
    "# -------------------------------------------------\n",
    "def load_dataset(X_path=\"dataset_X.txt\", y_path=\"dataset_y.txt\"):\n",
    "    X = np.loadtxt(X_path, dtype=complex)  # (N,4)\n",
    "    y_bits = np.loadtxt(y_path, dtype=int) # (N,4)\n",
    "    return X, y_bits\n",
    "\n",
    "\n",
    "def bits_to_int_labels(y_bits):\n",
    "    q1, q2, q3, q4 = (y_bits[:, i] for i in range(4))\n",
    "    return (q1 + 2*q2 + 4*q3 + 8*q4).astype(int)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Feature engineering\n",
    "# -------------------------------------------------\n",
    "def features_cartesian(Xc):\n",
    "    return np.hstack([Xc.real, Xc.imag])  # (N,8)\n",
    "\n",
    "\n",
    "def features_polar(Xc):\n",
    "    amp = np.abs(Xc)\n",
    "    phi = np.angle(Xc)\n",
    "    return np.hstack([amp, np.sin(phi), np.cos(phi)])  # (N,12)\n",
    "\n",
    "\n",
    "def features_cross(Xc):\n",
    "    feats = []\n",
    "    for i, j in combinations(range(4), 2):\n",
    "        prod = Xc[:, i] * np.conjugate(Xc[:, j])\n",
    "        feats.append(prod.real.reshape(-1, 1))\n",
    "        feats.append(prod.imag.reshape(-1, 1))\n",
    "    return np.hstack(feats)  # (N,12)\n",
    "\n",
    "\n",
    "def make_features(Xc, mode=\"polar_cross\"):\n",
    "    if mode == \"cartesian\":\n",
    "        return features_cartesian(Xc)\n",
    "    if mode == \"polar\":\n",
    "        return features_polar(Xc)\n",
    "    if mode == \"polar_cross\":\n",
    "        return np.hstack([features_polar(Xc), features_cross(Xc)])\n",
    "    raise ValueError(\"Invalid feature mode\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Linear UCB Bandit\n",
    "# -------------------------------------------------\n",
    "class LinearUCBBandit:\n",
    "    def __init__(self, n_actions, n_features, alpha=1.0):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.A = [np.eye(n_features) for _ in range(n_actions)]\n",
    "        self.b = [np.zeros(n_features) for _ in range(n_actions)]\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Select action using UCB rule.\n",
    "        \"\"\"\n",
    "        scores = np.zeros(self.n_actions)\n",
    "\n",
    "        for a in range(self.n_actions):\n",
    "            Ainv = np.linalg.inv(self.A[a])\n",
    "            theta = Ainv @ self.b[a]\n",
    "            mean = theta @ x\n",
    "            uncertainty = self.alpha * np.sqrt(x @ Ainv @ x)\n",
    "            scores[a] = mean + uncertainty\n",
    "\n",
    "        return np.argmax(scores)\n",
    "\n",
    "    def update(self, action, x, reward):\n",
    "        self.A[action] += np.outer(x, x)\n",
    "        self.b[action] += reward * x\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Training & evaluation\n",
    "# -------------------------------------------------\n",
    "def run_contextual_bandit(\n",
    "    feature_mode=\"polar_cross\",\n",
    "    alpha=1.0,\n",
    "    test_size=0.25,\n",
    "    seed=42\n",
    "):\n",
    "    # Load data\n",
    "    Xc, y_bits = load_dataset()\n",
    "    y = bits_to_int_labels(y_bits)\n",
    "\n",
    "    X = make_features(Xc, mode=feature_mode)\n",
    "\n",
    "    # Scale features (important!)\n",
    "    scaler = RobustScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    n_features = X.shape[1]\n",
    "    n_actions = 16\n",
    "\n",
    "    bandit = LinearUCBBandit(\n",
    "        n_actions=n_actions,\n",
    "        n_features=n_features,\n",
    "        alpha=alpha\n",
    "    )\n",
    "\n",
    "    # --- Offline training ---\n",
    "    for x, y_true in zip(X_train, y_train):\n",
    "        action = bandit.predict(x)\n",
    "        reward = 1.0 if action == y_true else 0.0\n",
    "        bandit.update(action, x, reward)\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    y_pred = np.array([bandit.predict(x) for x in X_test])\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n=== Contextual Bandit (Linear UCB) ===\")\n",
    "    print(f\"Feature mode : {feature_mode}\")\n",
    "    print(f\"Alpha        : {alpha}\")\n",
    "    print(f\"Accuracy     : {acc:.4f}\")\n",
    "\n",
    "    return acc, cm\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Try different alphas (exploration strength)\n",
    "    for alpha in [0.1, 0.3, 0.5, 1.0]:\n",
    "        run_contextual_bandit(\n",
    "            feature_mode=\"polar_cross\",\n",
    "            alpha=alpha\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489b2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17cf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MLP Multiclass (16-state) ===\n",
      "Feature mode : polar_cross\n",
      "Hidden layers: (128, 64)\n",
      "alpha (L2)   : 0.0001\n",
      "Accuracy     : 0.3810\n",
      "\n",
      "Classification report (macro avg is informative for balanced classes):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.316     0.648     0.425       250\n",
      "           1      0.325     0.344     0.334       250\n",
      "           2      0.341     0.440     0.384       250\n",
      "           3      0.420     0.368     0.392       250\n",
      "           4      0.331     0.448     0.381       250\n",
      "           5      0.505     0.404     0.449       250\n",
      "           6      0.396     0.380     0.388       250\n",
      "           7      0.431     0.248     0.315       250\n",
      "           8      0.424     0.500     0.459       250\n",
      "           9      0.420     0.284     0.339       250\n",
      "          10      0.379     0.376     0.378       250\n",
      "          11      0.437     0.348     0.388       250\n",
      "          12      0.438     0.420     0.429       250\n",
      "          13      0.327     0.324     0.325       250\n",
      "          14      0.353     0.380     0.366       250\n",
      "          15      0.505     0.184     0.270       250\n",
      "\n",
      "    accuracy                          0.381      4000\n",
      "   macro avg      0.397     0.381     0.376      4000\n",
      "weighted avg      0.397     0.381     0.376      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Experimental model for 4-qubit (16-class) readout classification using:\n",
    "  1) Angle (polar) encoding with sin/cos phase features\n",
    "  2) Cross-qubit interaction features Re(z_i * conj(z_j)), Im(z_i * conj(z_j))\n",
    "  3) A small MLP (FNN discriminator) with regularization\n",
    "\n",
    "Data:\n",
    "  - dataset_X.txt: (N,4) complex readouts\n",
    "  - dataset_y.txt: (N,4) bits [q1,q2,q3,q4], where q1 is the rightmost bit (LSB)\n",
    "\n",
    "This script:\n",
    "  - builds feature matrix X_feat\n",
    "  - scales it using RobustScaler\n",
    "  - trains an MLPClassifier for 16-class prediction\n",
    "  - reports accuracy + confusion matrix\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Load dataset\n",
    "# -------------------------------------------------\n",
    "def load_dataset(X_path=\"dataset_X.txt\", y_path=\"dataset_y.txt\"):\n",
    "    X = np.loadtxt(X_path, dtype=complex)   # (N,4)\n",
    "    y_bits = np.loadtxt(y_path, dtype=int)  # (N,4) with columns [q1,q2,q3,q4]\n",
    "    return X, y_bits\n",
    "\n",
    "\n",
    "def bits_to_int_labels(y_bits):\n",
    "    \"\"\"\n",
    "    Convert [q1,q2,q3,q4] -> integer 0..15, with q1 as LSB and q4 as MSB.\n",
    "    \"\"\"\n",
    "    q1, q2, q3, q4 = (y_bits[:, i] for i in range(4))\n",
    "    return (q1 + 2*q2 + 4*q3 + 8*q4).astype(int)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Feature engineering: Polar + Cross\n",
    "# -------------------------------------------------\n",
    "def polar_features(Xc):\n",
    "    \"\"\"\n",
    "    For each qubit z = I + iQ:\n",
    "      amp = |z|\n",
    "      phi = angle(z)\n",
    "    Use sin(phi), cos(phi) to avoid phase discontinuity.\n",
    "    Output dims: 4*(1+1+1) = 12\n",
    "      [amp1..amp4, sin(phi1)..sin(phi4), cos(phi1)..cos(phi4)]\n",
    "    \"\"\"\n",
    "    amp = np.abs(Xc)\n",
    "    phi = np.angle(Xc)\n",
    "    return np.hstack([amp, np.sin(phi), np.cos(phi)])\n",
    "\n",
    "\n",
    "def cross_features(Xc):\n",
    "    \"\"\"\n",
    "    Cross-qubit interaction features using z_i * conj(z_j).\n",
    "    For 4 qubits there are C(4,2)=6 pairs, each gives Re and Im => 12 dims.\n",
    "    \"\"\"\n",
    "    feats = []\n",
    "    for i, j in combinations(range(4), 2):\n",
    "        prod = Xc[:, i] * np.conjugate(Xc[:, j])\n",
    "        feats.append(prod.real.reshape(-1, 1))\n",
    "        feats.append(prod.imag.reshape(-1, 1))\n",
    "    return np.hstack(feats)  # (N,12)\n",
    "\n",
    "\n",
    "def build_features(Xc, mode=\"polar_cross\"):\n",
    "    if mode == \"polar\":\n",
    "        return polar_features(Xc)                 # (N,12)\n",
    "    if mode == \"polar_cross\":\n",
    "        return np.hstack([polar_features(Xc), cross_features(Xc)])  # (N,24)\n",
    "    if mode == \"cartesian\":\n",
    "        return np.hstack([Xc.real, Xc.imag])      # (N,8)\n",
    "    raise ValueError(\"mode must be 'cartesian'|'polar'|'polar_cross'\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Train & evaluate MLP for 16-class prediction\n",
    "# -------------------------------------------------\n",
    "def train_mlp_multiclass(\n",
    "    feature_mode=\"polar_cross\",\n",
    "    hidden_layers=(128, 64),\n",
    "    alpha=1e-4,\n",
    "    max_iter=600,\n",
    "    test_size=0.25,\n",
    "    seed=42\n",
    "):\n",
    "    # Load\n",
    "    Xc, y_bits = load_dataset()\n",
    "    y = bits_to_int_labels(y_bits)\n",
    "\n",
    "    # Features\n",
    "    X = build_features(Xc, mode=feature_mode)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = RobustScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    # MLP classifier\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layers,\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=alpha,          # L2 regularization strength\n",
    "        max_iter=max_iter,\n",
    "        random_state=seed,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=25,\n",
    "        validation_fraction=0.15\n",
    "    )\n",
    "\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n=== MLP Multiclass (16-state) ===\")\n",
    "    print(f\"Feature mode : {feature_mode}\")\n",
    "    print(f\"Hidden layers: {hidden_layers}\")\n",
    "    print(f\"alpha (L2)   : {alpha}\")\n",
    "    print(f\"Accuracy     : {acc:.4f}\")\n",
    "\n",
    "    # Optional: more detailed report\n",
    "    print(\"\\nClassification report (macro avg is informative for balanced classes):\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "    return mlp, scaler, acc, cm\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Quick benchmark: compare feature modes\n",
    "# -------------------------------------------------\n",
    "def benchmark_feature_modes():\n",
    "    for mode in [\"cartesian\", \"polar\", \"polar_cross\"]:\n",
    "        _, _, acc, _ = train_mlp_multiclass(\n",
    "            feature_mode=mode,\n",
    "            hidden_layers=(128, 64),\n",
    "            alpha=1e-4,\n",
    "            max_iter=700\n",
    "        )\n",
    "        print(f\"[Benchmark] mode={mode:12s}  acc={acc:.4f}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Run\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Train one strong candidate:\n",
    "    train_mlp_multiclass(\n",
    "        feature_mode=\"polar_cross\",\n",
    "        hidden_layers=(128, 64),\n",
    "        alpha=1e-4,\n",
    "        max_iter=700\n",
    "    )\n",
    "\n",
    "    # 2) Optional: compare all 3 feature modes quickly:\n",
    "    # benchmark_feature_modes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5bbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NeuralUCB for 4-qubit (16-class) readout classification\n",
    "Offline contextual bandit setting.\n",
    "\n",
    "- context x : feature vector from IQ readout\n",
    "- action a  : class in {0..15}\n",
    "- reward r  : 1 if correct, 0 otherwise\n",
    "\n",
    "Implements NeuralUCB with:\n",
    "  - shared neural network\n",
    "  - action encoded as one-hot\n",
    "  - Jacobian-based uncertainty estimate\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# -------------------------------------------------\n",
    "# Data & features\n",
    "# -------------------------------------------------\n",
    "def load_dataset(X_path=\"dataset_X.txt\", y_path=\"dataset_y.txt\"):\n",
    "    X = np.loadtxt(X_path, dtype=complex)\n",
    "    y_bits = np.loadtxt(y_path, dtype=int)\n",
    "    return X, y_bits\n",
    "\n",
    "def bits_to_int_labels(y_bits):\n",
    "    q1, q2, q3, q4 = (y_bits[:, i] for i in range(4))\n",
    "    return (q1 + 2*q2 + 4*q3 + 8*q4).astype(int)\n",
    "\n",
    "def features_polar(Xc):\n",
    "    amp = np.abs(Xc)\n",
    "    phi = np.angle(Xc)\n",
    "    return np.hstack([amp, np.sin(phi), np.cos(phi)])  # (N,12)\n",
    "\n",
    "def features_cross(Xc):\n",
    "    feats = []\n",
    "    for i, j in combinations(range(4), 2):\n",
    "        prod = Xc[:, i] * np.conjugate(Xc[:, j])\n",
    "        feats.append(prod.real.reshape(-1, 1))\n",
    "        feats.append(prod.imag.reshape(-1, 1))\n",
    "    return np.hstack(feats)  # (N,12)\n",
    "\n",
    "def make_features(Xc, mode=\"polar_cross\"):\n",
    "    if mode == \"polar_cross\":\n",
    "        return np.hstack([features_polar(Xc), features_cross(Xc)])\n",
    "    raise ValueError(\"Only polar_cross used for NeuralUCB\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Neural reward model\n",
    "# -------------------------------------------------\n",
    "class RewardNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden=(128, 64)):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden[0], hidden[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden[1], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# NeuralUCB Agent\n",
    "# -------------------------------------------------\n",
    "class NeuralUCB:\n",
    "    def __init__(self, context_dim, n_actions=16, alpha=1.0, lr=1e-3):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.model = RewardNet(context_dim + n_actions)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        # A = λI initially\n",
    "        self.params = list(self.model.parameters())\n",
    "        self.p_dim = sum(p.numel() for p in self.params)\n",
    "        self.A = torch.eye(self.p_dim)\n",
    "\n",
    "    def _one_hot(self, a):\n",
    "        v = torch.zeros(self.n_actions)\n",
    "        v[a] = 1.0\n",
    "        return v\n",
    "\n",
    "    def _forward(self, x, a):\n",
    "        xa = torch.cat([x, self._one_hot(a)])\n",
    "        return self.model(xa)\n",
    "\n",
    "    def _grad_vector(self, x, a):\n",
    "        self.optimizer.zero_grad()\n",
    "        r = self._forward(x, a)\n",
    "        r.backward()\n",
    "\n",
    "        grads = []\n",
    "        for p in self.model.parameters():\n",
    "            grads.append(p.grad.view(-1))\n",
    "        return torch.cat(grads).detach()\n",
    "\n",
    "    def select_action(self, x):\n",
    "        scores = []\n",
    "\n",
    "        Ainv = torch.inverse(self.A)\n",
    "\n",
    "        for a in range(self.n_actions):\n",
    "            with torch.no_grad():\n",
    "                mean = self._forward(x, a)\n",
    "\n",
    "            g = self._grad_vector(x, a)\n",
    "            uncert = torch.sqrt(g @ Ainv @ g)\n",
    "            score = mean + self.alpha * uncert\n",
    "            scores.append(score.item())\n",
    "\n",
    "        return int(np.argmax(scores))\n",
    "\n",
    "    def update(self, x, a, reward):\n",
    "        # Gradient for uncertainty\n",
    "        g = self._grad_vector(x, a)\n",
    "        self.A += torch.outer(g, g)\n",
    "\n",
    "        # Train reward model\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self._forward(x, a)\n",
    "        loss = self.loss_fn(pred, torch.tensor(reward))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Training & evaluation\n",
    "# -------------------------------------------------\n",
    "def run_neural_ucb(\n",
    "    alpha=0.5,\n",
    "    test_size=0.25,\n",
    "    seed=42\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    Xc, y_bits = load_dataset()\n",
    "    y = bits_to_int_labels(y_bits)\n",
    "\n",
    "    X = make_features(Xc, mode=\"polar_cross\")\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    Xtr = torch.tensor(X_train, dtype=torch.float32)\n",
    "    Xte = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    agent = NeuralUCB(\n",
    "        context_dim=Xtr.shape[1],\n",
    "        n_actions=16,\n",
    "        alpha=alpha\n",
    "    )\n",
    "\n",
    "    # --- Offline training ---\n",
    "    for x, y_true in zip(Xtr, y_train):\n",
    "        a = agent.select_action(x)\n",
    "        r = 1.0 if a == y_true else 0.0\n",
    "        agent.update(x, a, r)\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    preds = []\n",
    "    for x in Xte:\n",
    "        preds.append(agent.select_action(x))\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "    print(\"\\n=== NeuralUCB (offline) ===\")\n",
    "    print(f\"Alpha    : {alpha}\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "\n",
    "    return acc, cm\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main\n",
    "# -------------------------------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     for alpha in [0.1, 0.3, 0.5, 1.0]:\n",
    "#         run_neural_ucb(alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec54ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_checkpoint(agent, step, filename=\"neuralucb_ckpt.pt\"):\n",
    "    torch.save({\n",
    "        \"model_state\": agent.model.state_dict(),\n",
    "        \"A\": agent.A,\n",
    "        \"step\": step\n",
    "    }, filename)\n",
    "    print(f\"[Checkpoint saved at step {step}]\")\n",
    "\n",
    "def load_checkpoint(agent, filename=\"neuralucb_ckpt.pt\"):\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"[No checkpoint found — starting fresh]\")\n",
    "        return 0\n",
    "\n",
    "    ckpt = torch.load(filename, map_location=\"cpu\")\n",
    "    agent.model.load_state_dict(ckpt[\"model_state\"])\n",
    "    agent.A = ckpt[\"A\"]\n",
    "    step = ckpt[\"step\"]\n",
    "\n",
    "    print(f\"[Checkpoint loaded — resuming from step {step}]\")\n",
    "    return step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de0843fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_neural_ucb(\n",
    "    alpha=0.5,\n",
    "    test_size=0.25,\n",
    "    seed=42,\n",
    "    max_steps_per_run=50,     # ⬅️ چند شات در هر اجرا\n",
    "    checkpoint_file=\"neuralucb_ckpt.pt\"\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    Xc, y_bits = load_dataset()\n",
    "    y = bits_to_int_labels(y_bits)\n",
    "\n",
    "    X = make_features(Xc, mode=\"polar_cross\")\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    Xtr = torch.tensor(X_train, dtype=torch.float32)\n",
    "    Xte = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    agent = NeuralUCB(\n",
    "        context_dim=Xtr.shape[1],\n",
    "        n_actions=16,\n",
    "        alpha=alpha\n",
    "    )\n",
    "\n",
    "    # ⬇️ تلاش برای ادامه از checkpoint\n",
    "    start_step = load_checkpoint(agent, checkpoint_file)\n",
    "\n",
    "    # --------- Offline training (مرحله‌ای) ---------\n",
    "    end_step = min(start_step + max_steps_per_run, len(Xtr))\n",
    "\n",
    "    for i in range(start_step, end_step):\n",
    "        x = Xtr[i]\n",
    "        y_true = y_train[i]\n",
    "\n",
    "        a = agent.select_action(x)\n",
    "        r = 1.0 if a == y_true else 0.0\n",
    "        agent.update(x, a, r)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Training step {i}/{len(Xtr)}\")\n",
    "\n",
    "    # ⬇️ ذخیره‌ی امن\n",
    "    save_checkpoint(agent, end_step, checkpoint_file)\n",
    "\n",
    "    print(f\"[Stopped safely at step {end_step}]\")\n",
    "\n",
    "    # --------- Evaluation فقط وقتی آموزش کامل شد ---------\n",
    "    if end_step < len(Xtr):\n",
    "        print(\"Training not finished yet — skipping evaluation\")\n",
    "        return None, None\n",
    "\n",
    "    preds = []\n",
    "    for x in Xte:\n",
    "        preds.append(agent.select_action(x))\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "    print(\"\\n=== NeuralUCB (offline, finished) ===\")\n",
    "    print(f\"Alpha    : {alpha}\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "\n",
    "    return acc, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfeb6763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Checkpoint loaded — resuming from step 50]\n",
      "Training step 50/12000\n",
      "Training step 60/12000\n",
      "Training step 70/12000\n",
      "Training step 80/12000\n",
      "Training step 90/12000\n",
      "[Checkpoint saved at step 100]\n",
      "[Stopped safely at step 100]\n",
      "Training not finished yet — skipping evaluation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "run_neural_ucb(\n",
    "    alpha=0.5,\n",
    "    test_size=0.25,\n",
    "    seed=42,\n",
    "    max_steps_per_run=50,     # ⬅️ چند شات در هر اجرا\n",
    "    checkpoint_file=\"neuralucb_ckpt.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150274ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qcomputing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
